{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support():\n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "    from auton_survival import datasets\n",
    "    outcomes, features = datasets.load_support()\n",
    "    from auton_survival.preprocessing import Preprocessor\n",
    "    cat_feats = ['sex', 'dzgroup', 'dzclass', 'income', 'race', 'ca']\n",
    "    num_feats = ['age', 'num.co', 'meanbp', 'wblc', 'hrt', 'resp', \n",
    "            'temp', 'pafi', 'alb', 'bili', 'crea', 'sod', 'ph', \n",
    "                'glucose', 'bun', 'urine', 'adlp', 'adls']\n",
    "\n",
    "    features = Preprocessor().fit_transform(features, cat_feats=cat_feats, num_feats=num_feats)\n",
    "    x, t, e = features.values, outcomes.time.values, outcomes.event.values\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    tr_size = int(n*0.80)\n",
    "    te_size = int(n*0.20)\n",
    "\n",
    "    x_train, x_test = x[:tr_size], x[-te_size:]\n",
    "    t_train, t_test = t[:tr_size], t[-te_size:]\n",
    "    e_train, e_test = e[:tr_size], e[-te_size:]\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic():\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from tqdm import tqdm \n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "\n",
    "    from auton_survival.datasets import load_dataset\n",
    "\n",
    "    # Load the synthetic dataset\n",
    "    outcomes, features, interventions = load_dataset(dataset='SYNTHETIC')\n",
    "\n",
    "    # Hyper-parameters\n",
    "    random_seed = 0\n",
    "    test_size = 0.25\n",
    "\n",
    "    # Split the synthetic data into training and testing data\n",
    "    import numpy as np\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    n = features.shape[0] \n",
    "\n",
    "    test_idx = np.zeros(n).astype('bool')\n",
    "    test_idx[np.random.randint(n, size=int(n*test_size))] = True \n",
    "\n",
    "    features_tr = features.iloc[~test_idx] \n",
    "    outcomes_tr = outcomes.iloc[~test_idx]\n",
    "    interventions_tr = interventions[~test_idx]\n",
    "    print(f'Number of training data points: {len(features_tr)}')\n",
    "\n",
    "    features_te = features.iloc[test_idx] \n",
    "    outcomes_te = outcomes.iloc[test_idx]\n",
    "    interventions_te = interventions[test_idx]\n",
    "    print(f'Number of test data points: {len(features_te)}')\n",
    "\n",
    "    interventions_tr.name, interventions_te.name = 'treat', 'treat'\n",
    "    features_tr_dcph = pd.concat([features_tr, interventions_tr.astype('float64')], axis=1)\n",
    "    features_te_dcph = pd.concat([features_te, interventions_te.astype('float64')], axis=1)\n",
    "    outcomes_tr_dcph = pd.DataFrame(outcomes_tr, columns=['event', 'time']).astype('float64')\n",
    "\n",
    "\n",
    "    x_train = features_tr_dcph.values\n",
    "    e_train = outcomes_tr['event'].values.astype(float)\n",
    "    t_train = outcomes_tr['time'].values\n",
    "\n",
    "    x_test = features_te_dcph.values\n",
    "    e_test = outcomes_te['event'].values.astype(float)\n",
    "    t_test = outcomes_te['time'].values\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kkbox():\n",
    "    from pycox.datasets import from_kkbox\n",
    "\n",
    "\n",
    "    kkbox_data = from_kkbox._DatasetKKBoxChurn()\n",
    "    #kkbox_data.download_kkbox()\n",
    "\n",
    "    df = kkbox_data.read_df()\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    e = np.array(df.event)\n",
    "    t = np.array(df.duration)\n",
    "    x = df.drop(columns=['event','duration','msno'])\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    x['gender'] = le.fit_transform(x['gender'])\n",
    "    x['registered_via'] = le.fit_transform(x['registered_via'])\n",
    "    x['city'] = le.fit_transform(x['city'])\n",
    "    x = np.array(x).astype(float)\n",
    "\n",
    "    import os, sys\n",
    "    import numpy as np \n",
    "\n",
    "    # path = '/home/r10user10/Documents/Jiacheng/dspm-auton-survival'\n",
    "    # os.chdir(path)\n",
    "    # print(os.getcwd())\n",
    "\n",
    "    from auton_survival import datasets\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    tr_size = int(n * 0.80)\n",
    "    te_size = int(n * 0.20)\n",
    "\n",
    "\n",
    "    x_train, x_test = x[:tr_size], x[-te_size:]\n",
    "    t_train, t_test = t[:tr_size], t[-te_size:]\n",
    "    e_train, e_test = e[:tr_size], e[-te_size:]\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimic():\n",
    "    import numpy as np\n",
    "    x_train = np.load('x_train.npy')\n",
    "    t_train = np.load('t_train.npy')\n",
    "    e_train = 1 - np.load('e_train.npy')\n",
    "    index = np.where(t_train <= 0)[0]\n",
    "    t_train = np.delete(t_train, index)\n",
    "    e_train = np.delete(e_train, index)\n",
    "    x_train = np.delete(x_train, index, axis=0)\n",
    "    x_test = np.load('x_test.npy')\n",
    "    t_test = np.load('t_test.npy')\n",
    "    e_test = 1 - np.load('e_test.npy')\n",
    "    index = np.where(t_test <= 0)[0]\n",
    "    t_test = np.delete(t_test, index)\n",
    "    e_test = np.delete(e_test, index)\n",
    "    x_test = np.delete(x_test, index, axis=0)\n",
    "    x_train = np.mean(x_train, axis=1)\n",
    "    x_test = np.mean(x_test, axis=1)\n",
    "    print(x_train.shape)\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(baseline, dataset, lr, n_components):\n",
    "    if baseline == 'DeepCox':\n",
    "        from auton_survival import DeepCoxPH\n",
    "        model = DeepCoxPH(layers=[100,100])\n",
    "    if baseline == 'DSM':\n",
    "        from auton_survival.models.dsm import DeepSurvivalMachines\n",
    "        model = DeepSurvivalMachines(k = n_components,\n",
    "                                distribution = 'LogNormal',\n",
    "                                layers = [100,100])\n",
    "    if baseline == 'DCM':\n",
    "        from auton_survival.models.dcm import DeepCoxMixtures\n",
    "        model = DeepCoxMixtures(k = n_components, layers = [100,100])\n",
    "    if baseline == 'DDPSM':\n",
    "        from auton_survival.models.dpsm import DeepDP\n",
    "        model = DeepDP(k= n_components,\n",
    "               distribution='LogNormal',\n",
    "               layers=[100,100])\n",
    "\n",
    "    if dataset == 'support':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = support()\n",
    "    if dataset == 'synthetic':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = synthetic()\n",
    "    if dataset == 'kkbox':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = kkbox()\n",
    "    if dataset == 'mimic':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = mimic()   \n",
    "    \n",
    "    model.fit(x_train, t_train, e_train, iters = 100, learning_rate = lr)\n",
    "    horizons = [0.25, 0.5, 0.75, 0.9]\n",
    "    x = np.concatenate((x_train, x_test), axis=0)\n",
    "    t = np.concatenate((t_train, t_test), axis=0)\n",
    "    e = np.concatenate((e_train, e_test), axis=0)\n",
    "    times = np.quantile(t[e==1], horizons).tolist()\n",
    "    out_risk = 1 - model.predict_survival(x_test, times)\n",
    "    out_survival = model.predict_survival(x_test, times)\n",
    "    print(out_survival.shape)\n",
    "\n",
    "    from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc\n",
    "\n",
    "    cis = []\n",
    "    brs = []\n",
    "\n",
    "    et_train = np.array([(e_train[i], t_train[i]) for i in range(len(e_train))],\n",
    "                    dtype = [('e', bool), ('t', float)])\n",
    "    #print(et_train)\n",
    "    et_test = np.array([(e_test[i], t_test[i]) for i in range(len(e_test))],\n",
    "                    dtype = [('e', bool), ('t', float)])\n",
    "    # et_val = np.array([(e_val[i], t_val[i]) for i in range(len(e_val))],\n",
    "    #                  dtype = [('e', bool), ('t', float)])\n",
    "    # print(et_train[0:10])\n",
    "    for i, _ in enumerate(times):\n",
    "        cis.append(concordance_index_ipcw(et_train, et_test, out_risk[:, i], times[i])[0])\n",
    "    brs.append(brier_score(et_train, et_test, out_survival, times)[1])\n",
    "    roc_auc = []\n",
    "    for i, _ in enumerate(times):\n",
    "        roc_auc.append(cumulative_dynamic_auc(et_train, et_test, out_risk[:, i], times[i])[0])\n",
    "    for horizon in enumerate(horizons):\n",
    "        print(f\"For {horizon[1]} quantile\")\n",
    "        print(\"TD Concordance Index:\", cis[horizon[0]])\n",
    "        print(\"Brier Score:\", brs[0][horizon[0]])\n",
    "        print(\"ROC AUC \", roc_auc[horizon[0]][0], \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 3899\n",
      "Number of test data points: 1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 219/10000 [00:00<00:13, 710.00it/s]\n",
      " 81%|████████  | 81/100 [00:37<00:08,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1101, 4)\n",
      "For 0.25 quantile\n",
      "TD Concordance Index: 0.48677281783413356\n",
      "Brier Score: 0.1689927588458899\n",
      "ROC AUC  0.4797423911448183 \n",
      "\n",
      "For 0.5 quantile\n",
      "TD Concordance Index: 0.5312662746501339\n",
      "Brier Score: 0.25278298269630034\n",
      "ROC AUC  0.5241038335232568 \n",
      "\n",
      "For 0.75 quantile\n",
      "TD Concordance Index: 0.6149005673295352\n",
      "Brier Score: 0.19693181561052\n",
      "ROC AUC  0.7126287565863961 \n",
      "\n",
      "For 0.9 quantile\n",
      "TD Concordance Index: 0.593031626763448\n",
      "Brier Score: 0.10982346267266813\n",
      "ROC AUC  0.7373627932987978 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline('DDPSM', 'synthetic', 1e-2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7816/10000 [00:15<00:04, 490.42it/s]\n",
      "100%|██████████| 100/100 [00:46<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1821, 4)\n",
      "For 0.25 quantile\n",
      "TD Concordance Index: 0.5713739845835967\n",
      "Brier Score: 0.12295952325251332\n",
      "ROC AUC  0.5732462089125442 \n",
      "\n",
      "For 0.5 quantile\n",
      "TD Concordance Index: 0.5598684599759403\n",
      "Brier Score: 0.20589500385719195\n",
      "ROC AUC  0.5614805245025662 \n",
      "\n",
      "For 0.75 quantile\n",
      "TD Concordance Index: 0.5860827745531594\n",
      "Brier Score: 0.25139178883564955\n",
      "ROC AUC  0.6087647664627127 \n",
      "\n",
      "For 0.9 quantile\n",
      "TD Concordance Index: 0.5800076232572127\n",
      "Brier Score: 0.19718930442518315\n",
      "ROC AUC  0.636507048555511 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline('DDPSM', 'support', 1e-6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
