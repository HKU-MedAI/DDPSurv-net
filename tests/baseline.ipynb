{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support():\n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "    from auton_survival import datasets\n",
    "    outcomes, features = datasets.load_support()\n",
    "    from auton_survival.preprocessing import Preprocessor\n",
    "    cat_feats = ['sex', 'dzgroup', 'dzclass', 'income', 'race', 'ca']\n",
    "    num_feats = ['age', 'num.co', 'meanbp', 'wblc', 'hrt', 'resp', \n",
    "            'temp', 'pafi', 'alb', 'bili', 'crea', 'sod', 'ph', \n",
    "                'glucose', 'bun', 'urine', 'adlp', 'adls']\n",
    "\n",
    "    features = Preprocessor().fit_transform(features, cat_feats=cat_feats, num_feats=num_feats)\n",
    "    x, t, e = features.values, outcomes.time.values, outcomes.event.values\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    tr_size = int(n*0.80)\n",
    "    te_size = int(n*0.20)\n",
    "\n",
    "    x_train, x_test = x[:tr_size], x[-te_size:]\n",
    "    t_train, t_test = t[:tr_size], t[-te_size:]\n",
    "    e_train, e_test = e[:tr_size], e[-te_size:]\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic():\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from tqdm import tqdm \n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "\n",
    "    from auton_survival.datasets import load_dataset\n",
    "\n",
    "    # Load the synthetic dataset\n",
    "    outcomes, features, interventions = load_dataset(dataset='SYNTHETIC')\n",
    "\n",
    "    # Hyper-parameters\n",
    "    random_seed = 0\n",
    "    test_size = 0.25\n",
    "\n",
    "    # Split the synthetic data into training and testing data\n",
    "    import numpy as np\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    n = features.shape[0] \n",
    "\n",
    "    test_idx = np.zeros(n).astype('bool')\n",
    "    test_idx[np.random.randint(n, size=int(n*test_size))] = True \n",
    "\n",
    "    features_tr = features.iloc[~test_idx] \n",
    "    outcomes_tr = outcomes.iloc[~test_idx]\n",
    "    interventions_tr = interventions[~test_idx]\n",
    "    print(f'Number of training data points: {len(features_tr)}')\n",
    "\n",
    "    features_te = features.iloc[test_idx] \n",
    "    outcomes_te = outcomes.iloc[test_idx]\n",
    "    interventions_te = interventions[test_idx]\n",
    "    print(f'Number of test data points: {len(features_te)}')\n",
    "\n",
    "    interventions_tr.name, interventions_te.name = 'treat', 'treat'\n",
    "    features_tr_dcph = pd.concat([features_tr, interventions_tr.astype('float64')], axis=1)\n",
    "    features_te_dcph = pd.concat([features_te, interventions_te.astype('float64')], axis=1)\n",
    "    outcomes_tr_dcph = pd.DataFrame(outcomes_tr, columns=['event', 'time']).astype('float64')\n",
    "\n",
    "\n",
    "    x_train = features_tr_dcph.values\n",
    "    e_train = outcomes_tr['event'].values.astype(float)\n",
    "    t_train = outcomes_tr['time'].values\n",
    "\n",
    "    x_test = features_te_dcph.values\n",
    "    e_test = outcomes_te['event'].values.astype(float)\n",
    "    t_test = outcomes_te['time'].values\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kkbox():\n",
    "    from pycox.datasets import from_kkbox\n",
    "\n",
    "\n",
    "    kkbox_data = from_kkbox._DatasetKKBoxChurn()\n",
    "    #kkbox_data.download_kkbox()\n",
    "\n",
    "    df = kkbox_data.read_df()\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    e = np.array(df.event)\n",
    "    t = np.array(df.duration)\n",
    "    x = df.drop(columns=['event','duration','msno'])\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    x['gender'] = le.fit_transform(x['gender'])\n",
    "    x['registered_via'] = le.fit_transform(x['registered_via'])\n",
    "    x['city'] = le.fit_transform(x['city'])\n",
    "    x = np.array(x).astype(float)\n",
    "\n",
    "    import os, sys\n",
    "    import numpy as np \n",
    "\n",
    "    # path = '/home/r10user10/Documents/Jiacheng/dspm-auton-survival'\n",
    "    # os.chdir(path)\n",
    "    # print(os.getcwd())\n",
    "\n",
    "    from auton_survival import datasets\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    tr_size = int(n * 0.80)\n",
    "    te_size = int(n * 0.20)\n",
    "\n",
    "\n",
    "    x_train, x_test = x[:tr_size], x[-te_size:]\n",
    "    t_train, t_test = t[:tr_size], t[-te_size:]\n",
    "    e_train, e_test = e[:tr_size], e[-te_size:]\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimic():\n",
    "    import numpy as np\n",
    "    x_train = np.load('x_train.npy')\n",
    "    t_train = np.load('t_train.npy')\n",
    "    e_train = 1 - np.load('e_train.npy')\n",
    "    index = np.where(t_train <= 0)[0]\n",
    "    t_train = np.delete(t_train, index)\n",
    "    e_train = np.delete(e_train, index)\n",
    "    x_train = np.delete(x_train, index, axis=0)\n",
    "    x_test = np.load('x_test.npy')\n",
    "    t_test = np.load('t_test.npy')\n",
    "    e_test = 1 - np.load('e_test.npy')\n",
    "    index = np.where(t_test <= 0)[0]\n",
    "    t_test = np.delete(t_test, index)\n",
    "    e_test = np.delete(e_test, index)\n",
    "    x_test = np.delete(x_test, index, axis=0)\n",
    "    x_train = np.mean(x_train, axis=1)\n",
    "    x_test = np.mean(x_test, axis=1)\n",
    "    print(x_train.shape)\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(baseline, dataset, lr, n_components):\n",
    "    if baseline == 'DeepCox':\n",
    "        from auton_survival import DeepCoxPH\n",
    "        model = DeepCoxPH(layers=[100,100])\n",
    "    if baseline == 'DSM':\n",
    "        from auton_survival.models.dsm import DeepSurvivalMachines\n",
    "        model = DeepSurvivalMachines(k = n_components,\n",
    "                                distribution = 'LogNormal',\n",
    "                                layers = [100,100])\n",
    "    if baseline == 'DCM':\n",
    "        from auton_survival.models.dcm import DeepCoxMixtures\n",
    "        model = DeepCoxMixtures(k = n_components, layers = [100,100])\n",
    "    if baseline == 'DDPSM':\n",
    "        from auton_survival.models.dpsm import DeepDP\n",
    "        model = DeepDP(k=100,\n",
    "               distribution='LogNormal',\n",
    "               layers=[100,100])\n",
    "\n",
    "    if dataset == 'support':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = support()\n",
    "    if dataset == 'synthetic':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = synthetic()\n",
    "    if dataset == 'kkbox':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = kkbox()\n",
    "    if dataset == 'mimic':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = mimic()   \n",
    "    \n",
    "    model.fit(x_train, t_train, e_train, iters = 100, learning_rate = lr)\n",
    "    horizons = [0.25, 0.5, 0.75, 0.9]\n",
    "    x = np.concatenate((x_train, x_test), axis=0)\n",
    "    t = np.concatenate((t_train, t_test), axis=0)\n",
    "    e = np.concatenate((e_train, e_test), axis=0)\n",
    "    times = np.quantile(t[e==1], horizons).tolist()\n",
    "    out_risk = 1 - model.predict_survival(x_test, times)\n",
    "    out_survival = model.predict_survival(x_test, times)\n",
    "\n",
    "    from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc\n",
    "\n",
    "    cis = []\n",
    "    brs = []\n",
    "\n",
    "    et_train = np.array([(e_train[i], t_train[i]) for i in range(len(e_train))],\n",
    "                    dtype = [('e', bool), ('t', float)])\n",
    "    #print(et_train)\n",
    "    et_test = np.array([(e_test[i], t_test[i]) for i in range(len(e_test))],\n",
    "                    dtype = [('e', bool), ('t', float)])\n",
    "    # et_val = np.array([(e_val[i], t_val[i]) for i in range(len(e_val))],\n",
    "    #                  dtype = [('e', bool), ('t', float)])\n",
    "    # print(et_train[0:10])\n",
    "    for i, _ in enumerate(times):\n",
    "        cis.append(concordance_index_ipcw(et_train, et_test, out_risk[:, i], times[i])[0])\n",
    "    brs.append(brier_score(et_train, et_test, out_survival, times)[1])\n",
    "    roc_auc = []\n",
    "    for i, _ in enumerate(times):\n",
    "        roc_auc.append(cumulative_dynamic_auc(et_train, et_test, out_risk[:, i], times[i])[0])\n",
    "    for horizon in enumerate(horizons):\n",
    "        print(f\"For {horizon[1]} quantile\")\n",
    "        print(\"TD Concordance Index:\", cis[horizon[0]])\n",
    "        print(\"Brier Score:\", brs[0][horizon[0]])\n",
    "        print(\"ROC AUC \", roc_auc[horizon[0]][0], \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r10user10/Documents/anaconda3/envs/python310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  9%|▉         | 9/100 [05:12<47:09, 31.09s/it]  /home/r10user10/Documents/anaconda3/envs/python310/lib/python3.10/site-packages/auton_survival/models/dcm/dcm_utilities.py:58: RuntimeWarning: invalid value encountered in power\n",
      "  return spl(ts)**risks\n",
      "/home/r10user10/Documents/anaconda3/envs/python310/lib/python3.10/site-packages/auton_survival/models/dcm/dcm_utilities.py:53: RuntimeWarning: invalid value encountered in power\n",
      "  s0ts = (-risks)*(spl(ts)**(risks-1))\n",
      " 15%|█▌        | 15/100 [06:46<38:23, 27.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0.25 quantile\n",
      "TD Concordance Index: 0.7299298997084658\n",
      "Brier Score: 0.11638414362703886\n",
      "ROC AUC  0.7403551895176493 \n",
      "\n",
      "For 0.5 quantile\n",
      "TD Concordance Index: 0.6779586644841935\n",
      "Brier Score: 0.1955746576531894\n",
      "ROC AUC  0.6982433089596662 \n",
      "\n",
      "For 0.75 quantile\n",
      "TD Concordance Index: 0.6443325378584507\n",
      "Brier Score: 0.23819393430704217\n",
      "ROC AUC  0.6834512631005027 \n",
      "\n",
      "For 0.9 quantile\n",
      "TD Concordance Index: 0.6351865163000653\n",
      "Brier Score: 0.18778241568445597\n",
      "ROC AUC  0.6906946736304485 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline('DCM', 'support', 1e-2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "baseline() missing 1 required positional argument: 'n_components'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbaseline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDCM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msynthetic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: baseline() missing 1 required positional argument: 'n_components'"
     ]
    }
   ],
   "source": [
    "baseline('DCM', 'synthetic', 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'auton_survival'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbaseline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDDPSM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msupport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mbaseline\u001b[0;34m(baseline, dataset, lr, n_components)\u001b[0m\n\u001b[1;32m     12\u001b[0m     model \u001b[38;5;241m=\u001b[39m DeepCoxMixtures(k \u001b[38;5;241m=\u001b[39m n_components, layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m baseline \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDPSM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mauton_survival\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdpsm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepDP\n\u001b[1;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m DeepDP(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     16\u001b[0m            distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogNormal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m            layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1405\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1255\u001b[0m, in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1242\u001b[0m, in \u001b[0;36m_recalculate\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1238\u001b[0m, in \u001b[0;36m_get_parent_path\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'auton_survival'"
     ]
    }
   ],
   "source": [
    "baseline('DDPSM', 'support', 5e-3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbaseline\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDPSM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkkbox\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline' is not defined"
     ]
    }
   ],
   "source": [
    "baseline('DDPSM', 'kkbox', 1e-4, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
