{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support():\n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "    from auton_survival import datasets\n",
    "    outcomes, features = datasets.load_support()\n",
    "    from auton_survival.preprocessing import Preprocessor\n",
    "    cat_feats = ['sex', 'dzgroup', 'dzclass', 'income', 'race', 'ca']\n",
    "    num_feats = ['age', 'num.co', 'meanbp', 'wblc', 'hrt', 'resp', \n",
    "            'temp', 'pafi', 'alb', 'bili', 'crea', 'sod', 'ph', \n",
    "                'glucose', 'bun', 'urine', 'adlp', 'adls']\n",
    "\n",
    "    features = Preprocessor().fit_transform(features, cat_feats=cat_feats, num_feats=num_feats)\n",
    "    x, t, e = features.values, outcomes.time.values, outcomes.event.values\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    tr_size = int(n*0.80)\n",
    "    te_size = int(n*0.20)\n",
    "\n",
    "    x_train, x_test = x[:tr_size], x[-te_size:]\n",
    "    t_train, t_test = t[:tr_size], t[-te_size:]\n",
    "    e_train, e_test = e[:tr_size], e[-te_size:]\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic():\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from tqdm import tqdm \n",
    "    import sys\n",
    "    sys.path.append('../')\n",
    "\n",
    "    from auton_survival.datasets import load_dataset\n",
    "\n",
    "    # Load the synthetic dataset\n",
    "    outcomes, features, interventions = load_dataset(dataset='SYNTHETIC')\n",
    "\n",
    "    # Hyper-parameters\n",
    "    random_seed = 0\n",
    "    test_size = 0.25\n",
    "\n",
    "    # Split the synthetic data into training and testing data\n",
    "    import numpy as np\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    n = features.shape[0] \n",
    "\n",
    "    test_idx = np.zeros(n).astype('bool')\n",
    "    test_idx[np.random.randint(n, size=int(n*test_size))] = True \n",
    "\n",
    "    features_tr = features.iloc[~test_idx] \n",
    "    outcomes_tr = outcomes.iloc[~test_idx]\n",
    "    interventions_tr = interventions[~test_idx]\n",
    "    print(f'Number of training data points: {len(features_tr)}')\n",
    "\n",
    "    features_te = features.iloc[test_idx] \n",
    "    outcomes_te = outcomes.iloc[test_idx]\n",
    "    interventions_te = interventions[test_idx]\n",
    "    print(f'Number of test data points: {len(features_te)}')\n",
    "\n",
    "    interventions_tr.name, interventions_te.name = 'treat', 'treat'\n",
    "    features_tr_dcph = pd.concat([features_tr, interventions_tr.astype('float64')], axis=1)\n",
    "    features_te_dcph = pd.concat([features_te, interventions_te.astype('float64')], axis=1)\n",
    "    outcomes_tr_dcph = pd.DataFrame(outcomes_tr, columns=['event', 'time']).astype('float64')\n",
    "\n",
    "\n",
    "    x_train = features_tr_dcph.values\n",
    "    e_train = outcomes_tr['event'].values.astype(float)\n",
    "    t_train = outcomes_tr['time'].values\n",
    "\n",
    "    x_test = features_te_dcph.values\n",
    "    e_test = outcomes_te['event'].values.astype(float)\n",
    "    t_test = outcomes_te['time'].values\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kkbox():\n",
    "    from pycox.datasets import from_kkbox\n",
    "\n",
    "\n",
    "    kkbox_data = from_kkbox._DatasetKKBoxChurn()\n",
    "    #kkbox_data.download_kkbox()\n",
    "\n",
    "    df = kkbox_data.read_df()\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    e = np.array(df.event)\n",
    "    t = np.array(df.duration)\n",
    "    x = df.drop(columns=['event','duration','msno'])\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    x['gender'] = le.fit_transform(x['gender'])\n",
    "    x['registered_via'] = le.fit_transform(x['registered_via'])\n",
    "    x['city'] = le.fit_transform(x['city'])\n",
    "    x = np.array(x).astype(float)\n",
    "\n",
    "    import os, sys\n",
    "    import numpy as np \n",
    "\n",
    "    # path = '/home/r10user10/Documents/Jiacheng/dspm-auton-survival'\n",
    "    # os.chdir(path)\n",
    "    # print(os.getcwd())\n",
    "\n",
    "    from auton_survival import datasets\n",
    "\n",
    "    n = len(x)\n",
    "\n",
    "    tr_size = int(n * 0.80)\n",
    "    te_size = int(n * 0.20)\n",
    "\n",
    "\n",
    "    x_train, x_test = x[:tr_size], x[-te_size:]\n",
    "    t_train, t_test = t[:tr_size], t[-te_size:]\n",
    "    e_train, e_test = e[:tr_size], e[-te_size:]\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimic():\n",
    "    import numpy as np\n",
    "    x_train = np.load('x_train.npy')\n",
    "    t_train = np.load('t_train.npy')\n",
    "    e_train = 1 - np.load('e_train.npy')\n",
    "    index = np.where(t_train <= 0)[0]\n",
    "    t_train = np.delete(t_train, index)\n",
    "    e_train = np.delete(e_train, index)\n",
    "    x_train = np.delete(x_train, index, axis=0)\n",
    "    x_test = np.load('x_test.npy')\n",
    "    t_test = np.load('t_test.npy')\n",
    "    e_test = 1 - np.load('e_test.npy')\n",
    "    index = np.where(t_test <= 0)[0]\n",
    "    t_test = np.delete(t_test, index)\n",
    "    e_test = np.delete(e_test, index)\n",
    "    x_test = np.delete(x_test, index, axis=0)\n",
    "    x_train = np.mean(x_train, axis=1)\n",
    "    x_test = np.mean(x_test, axis=1)\n",
    "    print(x_train.shape)\n",
    "    return x_train, t_train , e_train, x_test, t_test , e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(baseline, dataset, lr, n_components):\n",
    "    if baseline == 'DeepCox':\n",
    "        from auton_survival import DeepCoxPH\n",
    "        model = DeepCoxPH(layers=[100,100])\n",
    "    if baseline == 'DSM':\n",
    "        from auton_survival.models.dsm import DeepSurvivalMachines\n",
    "        model = DeepSurvivalMachines(k = n_components,\n",
    "                                distribution = 'LogNormal',\n",
    "                                layers = [100,100])\n",
    "    if baseline == 'DCM':\n",
    "        from auton_survival.models.dcm import DeepCoxMixtures\n",
    "        model = DeepCoxMixtures(k = n_components, layers = [100,100])\n",
    "    if baseline == 'DDPSM':\n",
    "        from auton_survival.models.dpsm import DeepDP\n",
    "        model = DeepDP(k= n_components,\n",
    "               distribution='Weibull',\n",
    "               layers=[100,100])\n",
    "\n",
    "    if dataset == 'support':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = support()\n",
    "    if dataset == 'synthetic':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = synthetic()\n",
    "    if dataset == 'kkbox':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = kkbox()\n",
    "    if dataset == 'mimic':\n",
    "        x_train, t_train , e_train, x_test, t_test , e_test = mimic()   \n",
    "    \n",
    "    model.fit(x_train, t_train, e_train, iters = 100, learning_rate = lr)\n",
    "    horizons = [0.25, 0.5, 0.75, 0.9]\n",
    "    x = np.concatenate((x_train, x_test), axis=0)\n",
    "    t = np.concatenate((t_train, t_test), axis=0)\n",
    "    e = np.concatenate((e_train, e_test), axis=0)\n",
    "    times = np.quantile(t[e==1], horizons).tolist()\n",
    "    out_risk = 1 - model.predict_survival(x_test, times)\n",
    "    out_survival = model.predict_survival(x_test, times)\n",
    "    print(out_survival.shape)\n",
    "\n",
    "    from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc\n",
    "\n",
    "    cis = []\n",
    "    brs = []\n",
    "\n",
    "    et_train = np.array([(e_train[i], t_train[i]) for i in range(len(e_train))],\n",
    "                    dtype = [('e', bool), ('t', float)])\n",
    "    #print(et_train)\n",
    "    et_test = np.array([(e_test[i], t_test[i]) for i in range(len(e_test))],\n",
    "                    dtype = [('e', bool), ('t', float)])\n",
    "    # et_val = np.array([(e_val[i], t_val[i]) for i in range(len(e_val))],\n",
    "    #                  dtype = [('e', bool), ('t', float)])\n",
    "    # print(et_train[0:10])\n",
    "    for i, _ in enumerate(times):\n",
    "        cis.append(concordance_index_ipcw(et_train, et_test, out_risk[:, i], times[i])[0])\n",
    "    brs.append(brier_score(et_train, et_test, out_survival, times)[1])\n",
    "    roc_auc = []\n",
    "    for i, _ in enumerate(times):\n",
    "        roc_auc.append(cumulative_dynamic_auc(et_train, et_test, out_risk[:, i], times[i])[0])\n",
    "    for horizon in enumerate(horizons):\n",
    "        print(f\"For {horizon[1]} quantile\")\n",
    "        print(\"TD Concordance Index:\", cis[horizon[0]])\n",
    "        print(\"Brier Score:\", brs[0][horizon[0]])\n",
    "        print(\"ROC AUC \", roc_auc[horizon[0]][0], \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline('DDPSM', 'kkbox', 1e-6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1820/10000 [00:04<00:20, 397.56it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbaseline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDDPSM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msupport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 28\u001b[0m, in \u001b[0;36mbaseline\u001b[0;34m(baseline, dataset, lr, n_components)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmimic\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     26\u001b[0m     x_train, t_train , e_train, x_test, t_test , e_test \u001b[38;5;241m=\u001b[39m mimic()   \n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m horizons \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.9\u001b[39m]\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((x_train, x_test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Jiacheng/dspm-auton-survival/auton_survival/models/dpsm/__init__.py:259\u001b[0m, in \u001b[0;36mDPSMBase.fit\u001b[0;34m(self, x, t, e, vsize, val_data, iters, learning_rate, batch_size, elbo, optimizer)\u001b[0m\n\u001b[1;32m    257\u001b[0m maxrisk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mnanmax(e_train\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m    258\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_torch_model(inputdim, optimizer, risks\u001b[38;5;241m=\u001b[39mmaxrisk)\n\u001b[0;32m--> 259\u001b[0m model, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dpsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m                      \u001b[49m\u001b[43melbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Jiacheng/dspm-auton-survival/auton_survival/models/dpsm/utilities.py:172\u001b[0m, in \u001b[0;36mtrain_dpsm\u001b[0;34m(model, x_train, t_train, e_train, x_valid, t_valid, e_valid, n_iter, lr, elbo, bs, random_seed)\u001b[0m\n\u001b[1;32m    170\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model\u001b[38;5;241m.\u001b[39mrisks):\n\u001b[0;32m--> 172\u001b[0m   loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mconditional_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_reshape_tensor_with_nans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_reshape_tensor_with_nans\u001b[49m\u001b[43m(\u001b[49m\u001b[43meb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43melbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mrisk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#print (\"Train Loss:\", float(loss))\u001b[39;00m\n\u001b[1;32m    179\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Jiacheng/dspm-auton-survival/auton_survival/models/dpsm/losses.py:298\u001b[0m, in \u001b[0;36mconditional_loss\u001b[0;34m(model, x, t, e, elbo, risk)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconditional_loss\u001b[39m(model, x, t, e, elbo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, risk\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdist \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeibull\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 298\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_conditional_weibull_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melbo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrisk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdist \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogNormal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _conditional_lognormal_loss(model, x, t, e, elbo, risk)\n",
      "File \u001b[0;32m~/Documents/Jiacheng/dspm-auton-survival/auton_survival/models/dpsm/losses.py:275\u001b[0m, in \u001b[0;36m_conditional_weibull_loss\u001b[0;34m(model, x, t, e, elbo, risk)\u001b[0m\n\u001b[1;32m    271\u001b[0m lossf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(lossf, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elbo:\n\u001b[0;32m--> 275\u001b[0m     lossg \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     losss \u001b[38;5;241m=\u001b[39m lossg \u001b[38;5;241m*\u001b[39m losss\n\u001b[1;32m    277\u001b[0m     lossf \u001b[38;5;241m=\u001b[39m lossg \u001b[38;5;241m*\u001b[39m lossf\n",
      "File \u001b[0;32m~/Documents/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1841\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "baseline('DDPSM', 'support', 1e-6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
